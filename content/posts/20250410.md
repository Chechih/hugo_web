---
title: "使用 Ollama 在本地部署 LLM 模型 By Ubuntu"
date: 2025-04-10T23:33:17+08:00
categories: ["Ollama", "DeepSeek" ]
---
### 前言
本文將介紹使用 Ollama 在本地端搭建 deepseek-r1:1.5b，因為小弟電腦配備比較差一點，這個搭建起來比較不會 Lag，
但如果電腦比較高級的可以到[官網](https://ollama.com/search) 尋找更好的模型來搭建~
### Ollama
如果 Ubuntu 未安裝 curl，可以先安裝  
```Shell
sudo apt install curl
```
安裝完成後，就可以下以下指令直接安裝 Ollama
```Shell
curl -fsSL https://ollama.com/install.sh | sh
```
安裝完成後，就可以直接執行以下指令，將 deepseek-r1:1.5b，執行起來了，如果要更換模型也是這段會不相同，  
備註: 未安裝的模型會先下載，記得確認下硬碟空間!!!
```Shell
ollama run deepseek-r1:1.5b
```
![Ollama Run deepseek-r1:1.5b](/images/20250410/1.jpg "run_deepseek")  

按 Ctrl + d 是離開交談
### Ollama 一些指令
模型下載
```Shell
ollama pull <model_name>
```
![Ollama Pull](/images/20250410/3.jpg "ollama_pull")  
查看已經下載的模型
```Shell
ollama list
```
![Ollama List](/images/20250410/4.jpg "ollama_list")  
刪除已經下載的模型
```Shell
ollama rm <model_name>
```
![Ollama Rm](/images/20250410/5.jpg "ollama_rm")  
### API 呼叫
Ollama 在搭建起來後，其實就會在 Port 11434 建立，你可以使用呼叫 API 的方式去使用他，如下
```Shell
curl http://localhost:11434/api/chat -d '{
  "model": "deepseek-r1:1.5b",
  "messages": [
    { "role": "user", "content": "2 + 2 是多少?" }
  ]
}'
```
因為我現在使用的模型是思考型的模型，呼叫後，他會回傳他思考的訊息回來，如下
![API Call deepseek-r1:1.5b](/images/20250410/2.jpg "api_call_deepseek")  
### API 呼叫 中的 messages 參數
1. 保留記憶的對話方式
```Json
"messages": [
    { "role": "user", "content": "2 + 2 是多少?" },
    { "role": "assistant", "content": "4" },
    { "role": "user", "content": "再乘以 3 呢?" }
]
```
2. 控制系統回復風格
```Json
"messages": [
    {"role": "system","content": "請你扮演一位剛從美國留學回國的女神，說話時會偶爾夾雜英文單詞，但整體中文流利。你優雅自信，見多識廣，對時尚、藝術和生活方式有獨到見解。你的談吐中流露出高水準的品味和優越感，但不會顯得咄咄逼人。記住，你是女神，所以要保持神秘感和距離感。"}.
	{ "role": "role": "user", "content": "美國的飲食還習慣麼？" }
]
```
3. 一般對話(無記憶)
```Json
"messages": [
    { "role": "user", "content": "如何學習 Python?" }
]
```
### 開放 Ollama 的 Port
Ollama 的 API 預設只能使用 localhost 呼叫，如果想開放給外部用需要修改設定
1. 停止服務
```Shell
sudo systemctl edit ollama
```
2. 修改 /etc/systemd/system/ollama.service 檔案
```Shell
vim /etc/systemd/system/ollama.service
```
3. 加入：
```INI
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
```
![Profile](/images/20250410/6.jpg "profile")  
4. 重新啟動：
```Shell
sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### 參考
- [使用Ollama本地部署DeepSeek R1模型：从零到精通的完整指南](https://deepseek.csdn.net/67abf75e82931a478c54afa8.html)  
- [五分鐘上手 Ollama - 在本機跑 LLM 語言模型](https://ywctech.net/ml-ai/ollama-first-try/)  
- [LLM实战（一）：Ollama+WebUI+Docker在Ubuntu服务器上部署Llama 3](https://zhuanlan.zhihu.com/p/696539735)  
- [DeepSeek 本地部署详细教程！](https://zhuanlan.zhihu.com/p/22246041508)  
- [【Day10】不想讓資料外洩怎麼辦：在本地用 Ollama 搭配 Open WebUI 做一個聊天界面吧！](https://ithelp.ithome.com.tw/articles/10357750)  
- [Ubuntu20.04 私有化部署 Ollama + DeepSeek + Dify，构建你的专属私人 AI 助手](https://www.cnblogs.com/odesey/p/18702141)  